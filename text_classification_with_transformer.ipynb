{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_with_transformer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.6 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "df955ce39d0f31d56d4bb2fe0a613e5326ba60723fd33d8303a3aede8f65715c"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXt6DHdpiBm4"
      },
      "source": [
        "# Text classification with Transformer\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/10<br>\n",
        "**Last modified:** 2020/05/10<br>\n",
        "**Description:** Implement a Transformer block as a Keras layer and use it for text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMwzgA_riBm_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1j5feYNiBm_"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDqLp29RiBnA"
      },
      "source": [
        "## Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjvX9WgHiBnA"
      },
      "source": [
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtlCZcEviBnB"
      },
      "source": [
        "## Implement embedding layer\n",
        "\n",
        "Two seperate embedding layers, one for tokens, one for token index (positions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuAQ4lzLiBnB"
      },
      "source": [
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRVYfXBuiBnC"
      },
      "source": [
        "## Load CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(path=\"all_only_system_metric_20201120_integrated_9_features.csv\"):\n",
        "    df = pandas.read_csv(path,nrows=200)\n",
        "    x = df.iloc[:,:-1].to_numpy(dtype='float16')\n",
        "    y = df.iloc[:,-1].to_numpy(dtype='int32')\n",
        "    x=[i.tolist() for i in x]\n",
        "    x = np.array(x)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3,random_state = 120)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ]
    },
    {
      "source": [
        "## Prepare dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izD01_gCiBnC",
        "tags": []
      },
      "source": [
        "  # Only consider the top 20k words\n",
        "maxlen = 9  # Only consider the first 200 pip install nbconvertwords of each movie review\n",
        "# (x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "(x_train, y_train), (x_val, y_val) = load_data()\n",
        "print( \"Training sequences\",len(x_train))\n",
        "print( \"Validation sequences\",len(x_val))\n",
        "# print(\"Training y_tranValue: \",y_train)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "# print(\"Training x_tranValue: \",x_train)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "vocab_size = max(map(max, x_train))+1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sequences 140\nValidation sequences 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCe_qTLDiBnC"
      },
      "source": [
        "## Create classifier model using transformer layer\n",
        "\n",
        "Transformer layer outputs one vector for each time step of our input sequence.\n",
        "Here, we take the mean across all time steps and\n",
        "use a feed forward network on top of it to classify text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUWa4uRDiBnD"
      },
      "source": [
        "\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7nlyazKiBnD"
      },
      "source": [
        "## Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMCZwmIGiBnD"
      },
      "source": [
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=50, validation_data=(x_val, y_val))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 4s 369ms/step - loss: 0.6791 - accuracy: 0.5841 - val_loss: 0.6189 - val_accuracy: 0.7167\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.5958 - accuracy: 0.6693 - val_loss: 0.5605 - val_accuracy: 0.7167\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 0.4801 - accuracy: 0.7394 - val_loss: 0.5261 - val_accuracy: 0.7333\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.4313 - accuracy: 0.7738 - val_loss: 0.4997 - val_accuracy: 0.7333\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.3280 - accuracy: 0.8584 - val_loss: 0.4630 - val_accuracy: 0.8000\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.2418 - accuracy: 0.9467 - val_loss: 0.4354 - val_accuracy: 0.8500\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.1843 - accuracy: 0.9642 - val_loss: 0.4593 - val_accuracy: 0.8167\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.1328 - accuracy: 0.9551 - val_loss: 0.4056 - val_accuracy: 0.8500\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.4324 - val_accuracy: 0.8500\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.4985 - val_accuracy: 0.8500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}